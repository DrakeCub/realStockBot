{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8260eb",
   "metadata": {},
   "source": [
    "This is only to test branches. No worries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee750cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\caleb\\.cache\\kagglehub\\datasets\\borismarjanovic\\price-volume-data-for-all-us-stocks-etfs\\versions\\3\n"
     ]
    }
   ],
   "source": [
    "#Dowloading the data from kaggle. I used this website:\n",
    "#https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs?select=Stocks\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"borismarjanovic/price-volume-data-for-all-us-stocks-etfs\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d68ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7163 tickers into dataframe of shape: (14887665, 8)\n",
      "        Date    Open    High     Low   Close    Volume  OpenInt Ticker\n",
      "0 1999-11-18  30.713  33.754  27.002  29.702  66277506        0   a.us\n",
      "1 1999-11-19  28.986  29.027  26.872  27.257  16142920        0   a.us\n",
      "2 1999-11-22  27.886  29.702  27.044  29.702   6970266        0   a.us\n",
      "3 1999-11-23  28.688  29.446  27.002  27.002   6332082        0   a.us\n",
      "4 1999-11-24  27.083  28.309  27.002  27.717   5132147        0   a.us\n"
     ]
    }
   ],
   "source": [
    "#Combining all the data from the stock folder into one dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get path to 'stocks' folder\n",
    "stocks_path = os.path.join(path, \"stocks\")\n",
    "txt_files = glob.glob(os.path.join(stocks_path, \"*.txt\"))\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    ticker = os.path.basename(file).replace(\".txt\", \"\")\n",
    "    \n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            header = f.readline().strip()\n",
    "            if not header or ',' not in header:\n",
    "                continue  # Skip files without a proper header\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "        if df.empty or len(df.columns) < 6:\n",
    "            continue  # Skip empty or malformed data\n",
    "\n",
    "        df['Ticker'] = ticker\n",
    "        dfs.append(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {ticker}: {e}\")\n",
    "\n",
    "# Combine all successfully loaded files\n",
    "stock_data = pd.concat(dfs, ignore_index=True)\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "print(f\"Loaded {len(dfs)} tickers into dataframe of shape: {stock_data.shape}\")\n",
    "print(stock_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefe00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (11587794, 8)\n",
      "Unique tickers remaining: 4097\n",
      "        Date    Open    High     Low   Close    Volume  OpenInt Ticker\n",
      "0 1999-11-18  30.713  33.754  27.002  29.702  66277506        0   a.us\n",
      "1 1999-11-19  28.986  29.027  26.872  27.257  16142920        0   a.us\n",
      "2 1999-11-22  27.886  29.702  27.044  29.702   6970266        0   a.us\n",
      "3 1999-11-23  28.688  29.446  27.002  27.002   6332082        0   a.us\n",
      "4 1999-11-24  27.083  28.309  27.002  27.717   5132147        0   a.us\n"
     ]
    }
   ],
   "source": [
    "#Filtering stocks by 1. Too little data (<2 yrs). or 2. Low trading volume\n",
    "# Count the number of rows per ticker\n",
    "min_days = 500\n",
    "ticker_counts = stock_data['Ticker'].value_counts()\n",
    "\n",
    "# Filter to include only tickers with at least min_days of data\n",
    "valid_tickers = ticker_counts[ticker_counts >= min_days].index\n",
    "filtered_data = stock_data[stock_data['Ticker'].isin(valid_tickers)]\n",
    "\n",
    "# Compute average volume per ticker\n",
    "avg_volume = filtered_data.groupby('Ticker')['Volume'].mean()\n",
    "\n",
    "# Keep only tickers above a threshold\n",
    "min_volume = 50000\n",
    "liquid_tickers = avg_volume[avg_volume >= min_volume].index\n",
    "filtered_data = filtered_data[filtered_data['Ticker'].isin(liquid_tickers)]\n",
    "\n",
    "print(f\"Final dataset shape: {filtered_data.shape}\")\n",
    "print(f\"Unique tickers remaining: {filtered_data['Ticker'].nunique()}\")\n",
    "print(filtered_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Feature engineering\n",
    "# Assume filtered_data is already sorted by Ticker and Date\n",
    "filtered_data = filtered_data.sort_values(['Ticker', 'Date']).copy()\n",
    "\n",
    "# Group by each stock ticker\n",
    "grouped = filtered_data.groupby('Ticker', group_keys=False)\n",
    "\n",
    "# Feature 1: Log Return\n",
    "filtered_data['LogReturn'] = grouped['Close'].apply(lambda x: np.log(x / x.shift(1)))\n",
    "\n",
    "# Feature 2: 10-Day Moving Average of Close\n",
    "filtered_data['MA10'] = grouped['Close'].apply(lambda x: x.rolling(window=10).mean())\n",
    "\n",
    "# Feature 3: 10-Day Volatility (Std Dev of Log Returns)\n",
    "filtered_data['Volatility10'] = grouped['LogReturn'].apply(lambda x: x.rolling(window=10).std())\n",
    "print(filtered_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6874f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay! No more NaNs! \n",
      "\n",
      "<bound method NDFrame.head of                Date    Open     High      Low   Close   Volume  OpenInt  \\\n",
      "10       1999-12-03  30.336  30.8420  29.9090  30.039  3223074        0   \n",
      "11       1999-12-06  30.547  31.3480  30.5050  30.883  2385046        0   \n",
      "12       1999-12-07  30.883  31.0520  29.9090  30.547  2348161        0   \n",
      "13       1999-12-08  30.547  30.7950  30.2490  30.505  2000481        0   \n",
      "14       1999-12-09  30.547  31.0120  30.5470  30.924  2150096        0   \n",
      "...             ...     ...      ...      ...     ...      ...      ...   \n",
      "14887660 2017-11-06  10.420  11.5400  10.4200  11.190   977948        0   \n",
      "14887661 2017-11-07  11.300  11.4200  10.6700  10.830   451210        0   \n",
      "14887662 2017-11-08  10.700  11.0600  10.3500  10.900   336449        0   \n",
      "14887663 2017-11-09  11.000  11.8563  10.9700  11.600   463067        0   \n",
      "14887664 2017-11-10  11.680  13.1500  11.3043  12.460   885587        0   \n",
      "\n",
      "           Ticker  LogReturn     MA10  Volatility10  \n",
      "10           a.us   0.008458  28.5208      0.053895  \n",
      "11           a.us   0.027709  28.8834      0.044701  \n",
      "12           a.us  -0.010939  28.9679      0.036825  \n",
      "13           a.us  -0.001376  29.3182      0.013804  \n",
      "14           a.us   0.013642  29.6389      0.012940  \n",
      "...           ...        ...      ...           ...  \n",
      "14887660  zyne.us   0.079968   9.9970      0.035647  \n",
      "14887661  zyne.us  -0.032700  10.0750      0.038328  \n",
      "14887662  zyne.us   0.006443  10.1650      0.038084  \n",
      "14887663  zyne.us   0.062242  10.3750      0.035041  \n",
      "14887664  zyne.us   0.071518  10.6410      0.038616  \n",
      "\n",
      "[11546824 rows x 11 columns]>\n"
     ]
    }
   ],
   "source": [
    "#At this point you will see that some of the engineered features are NaN.\n",
    "#Why? For a log return, it is because there is no prior close, so it can't calculate the percentage change from one day to the next\n",
    "#For MA10, it's because there must be at least 10 samples to calculate the smoothed average over 10 days. Hence the first 9 entries are NaN\n",
    "#For Volatility10, it needs 11 total entries to perfrom the calculation of degree of variation in returns. \n",
    "\n",
    "#Fix by deleting entries with NaN. Apparently it's 'standard practice' \n",
    "filtered_data = filtered_data.dropna(subset=['LogReturn', 'MA10', 'Volatility10'])\n",
    "print(\"Yay! No more NaNs! \\n\")\n",
    "print(filtered_data.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a195d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RSI feature for calculating if stock is overbought or oversold\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "filtered_data['RSI14'] = (\n",
    "    filtered_data.groupby('Ticker')['Close'].transform(lambda x: compute_rsi(x, 14))\n",
    ")\n",
    "#Will come back to this and see if it affects the outcome\n",
    "# #MACD feature to detect if the market is bear/bull market\n",
    "# def compute_macd(series, fast=12, slow=26, signal=9):\n",
    "#     ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "#     ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "#     macd = ema_fast - ema_slow\n",
    "#     signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "#     hist = macd - signal_line\n",
    "#     return pd.DataFrame({\n",
    "#         'MACD': macd,\n",
    "#         'MACD_Signal': signal_line,\n",
    "#         'MACD_Hist': hist\n",
    "#     }, index=series.index)\n",
    "\n",
    "# # Apply per-ticker and assign result to the main DataFrame\n",
    "# macd_df = filtered_data.groupby('Ticker')['Close'].apply(compute_macd).reset_index()\n",
    "\n",
    "# # Merge back into the main DataFrame\n",
    "# filtered_data = filtered_data.merge(macd_df, on=['Ticker', 'level_1'])  # 'level_1' is the original Date index\n",
    "# filtered_data = filtered_data.rename(columns={'level_1': 'Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "645a77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have features, it's time to create some juicy labels to say if the stock price went up that day. \n",
    "#This can be twofold a problem because we can either make it a classifier(should we sell or not) or a reggressor (how much do we sell + or -)\n",
    "#Classifier to start\n",
    "\n",
    "filtered_data = filtered_data.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# Grouped shift to get next-day close price\n",
    "filtered_data['NextClose'] = filtered_data.groupby('Ticker')['Close'].shift(-1)\n",
    "\n",
    "# Compute next-day log return\n",
    "filtered_data['NextLogReturn'] = np.log(filtered_data['NextClose'] / filtered_data['Close'])\n",
    "\n",
    "# 1 if next day's return is positive, else 0\n",
    "filtered_data['Target'] = (filtered_data['NextLogReturn'] > 0).astype(int)\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['NextLogReturn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6a5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
