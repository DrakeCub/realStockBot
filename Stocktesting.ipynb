{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8260eb",
   "metadata": {},
   "source": [
    "This is only to test branches. No worries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee750cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "Path to dataset files: C:\\Users\\caleb\\.cache\\kagglehub\\datasets\\borismarjanovic\\price-volume-data-for-all-us-stocks-etfs\\versions\\3\n"
     ]
    }
   ],
   "source": [
    "#Dowloading the data from kaggle. I used this website:\n",
    "#https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs?select=Stocks\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"borismarjanovic/price-volume-data-for-all-us-stocks-etfs\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98d68ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7163 tickers into dataframe of shape: (14887665, 8)\n",
      "        Date    Open    High     Low   Close    Volume  OpenInt Ticker\n",
      "0 1999-11-18  30.713  33.754  27.002  29.702  66277506        0   a.us\n",
      "1 1999-11-19  28.986  29.027  26.872  27.257  16142920        0   a.us\n",
      "2 1999-11-22  27.886  29.702  27.044  29.702   6970266        0   a.us\n",
      "3 1999-11-23  28.688  29.446  27.002  27.002   6332082        0   a.us\n",
      "4 1999-11-24  27.083  28.309  27.002  27.717   5132147        0   a.us\n"
     ]
    }
   ],
   "source": [
    "#Combining all the data from the stock folder into one dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get path to 'stocks' folder\n",
    "stocks_path = os.path.join(path, \"stocks\")\n",
    "txt_files = glob.glob(os.path.join(stocks_path, \"*.txt\"))\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    ticker = os.path.basename(file).replace(\".txt\", \"\")\n",
    "    \n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            header = f.readline().strip()\n",
    "            if not header or ',' not in header:\n",
    "                continue  # Skip files without a proper header\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "        if df.empty or len(df.columns) < 6:\n",
    "            continue  # Skip empty or malformed data\n",
    "\n",
    "        df['Ticker'] = ticker\n",
    "        dfs.append(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {ticker}: {e}\")\n",
    "\n",
    "# Combine all successfully loaded files\n",
    "stock_data = pd.concat(dfs, ignore_index=True)\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "print(f\"Loaded {len(dfs)} tickers into dataframe of shape: {stock_data.shape}\")\n",
    "print(stock_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eefe00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (11587794, 8)\n",
      "Unique tickers remaining: 4097\n",
      "        Date    Open    High     Low   Close    Volume  OpenInt Ticker\n",
      "0 1999-11-18  30.713  33.754  27.002  29.702  66277506        0   a.us\n",
      "1 1999-11-19  28.986  29.027  26.872  27.257  16142920        0   a.us\n",
      "2 1999-11-22  27.886  29.702  27.044  29.702   6970266        0   a.us\n",
      "3 1999-11-23  28.688  29.446  27.002  27.002   6332082        0   a.us\n",
      "4 1999-11-24  27.083  28.309  27.002  27.717   5132147        0   a.us\n"
     ]
    }
   ],
   "source": [
    "#Filtering stocks by 1. Too little data (<2 yrs). or 2. Low trading volume\n",
    "# Count the number of rows per ticker\n",
    "min_days = 500\n",
    "ticker_counts = stock_data['Ticker'].value_counts()\n",
    "\n",
    "# Filter to include only tickers with at least min_days of data\n",
    "valid_tickers = ticker_counts[ticker_counts >= min_days].index\n",
    "filtered_data = stock_data[stock_data['Ticker'].isin(valid_tickers)]\n",
    "\n",
    "# Compute average volume per ticker\n",
    "avg_volume = filtered_data.groupby('Ticker')['Volume'].mean()\n",
    "\n",
    "# Keep only tickers above a threshold\n",
    "min_volume = 50000\n",
    "liquid_tickers = avg_volume[avg_volume >= min_volume].index\n",
    "filtered_data = filtered_data[filtered_data['Ticker'].isin(liquid_tickers)]\n",
    "\n",
    "print(f\"Final dataset shape: {filtered_data.shape}\")\n",
    "print(f\"Unique tickers remaining: {filtered_data['Ticker'].nunique()}\")\n",
    "print(filtered_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2951a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                Date    Open     High      Low   Close    Volume  OpenInt  \\\n",
      "0        1999-11-18  30.713  33.7540  27.0020  29.702  66277506        0   \n",
      "1        1999-11-19  28.986  29.0270  26.8720  27.257  16142920        0   \n",
      "2        1999-11-22  27.886  29.7020  27.0440  29.702   6970266        0   \n",
      "3        1999-11-23  28.688  29.4460  27.0020  27.002   6332082        0   \n",
      "4        1999-11-24  27.083  28.3090  27.0020  27.717   5132147        0   \n",
      "...             ...     ...      ...      ...     ...       ...      ...   \n",
      "14887660 2017-11-06  10.420  11.5400  10.4200  11.190    977948        0   \n",
      "14887661 2017-11-07  11.300  11.4200  10.6700  10.830    451210        0   \n",
      "14887662 2017-11-08  10.700  11.0600  10.3500  10.900    336449        0   \n",
      "14887663 2017-11-09  11.000  11.8563  10.9700  11.600    463067        0   \n",
      "14887664 2017-11-10  11.680  13.1500  11.3043  12.460    885587        0   \n",
      "\n",
      "           Ticker  LogReturn    MA10  Volatility10  \n",
      "0            a.us        NaN     NaN           NaN  \n",
      "1            a.us  -0.085904     NaN           NaN  \n",
      "2            a.us   0.085904     NaN           NaN  \n",
      "3            a.us  -0.095303     NaN           NaN  \n",
      "4            a.us   0.026135     NaN           NaN  \n",
      "...           ...        ...     ...           ...  \n",
      "14887660  zyne.us   0.079968   9.997      0.035647  \n",
      "14887661  zyne.us  -0.032700  10.075      0.038328  \n",
      "14887662  zyne.us   0.006443  10.165      0.038084  \n",
      "14887663  zyne.us   0.062242  10.375      0.035041  \n",
      "14887664  zyne.us   0.071518  10.641      0.038616  \n",
      "\n",
      "[11587794 rows x 11 columns]>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Feature engineering\n",
    "# Assume filtered_data is already sorted by Ticker and Date\n",
    "filtered_data = filtered_data.sort_values(['Ticker', 'Date']).copy()\n",
    "\n",
    "# Group by each stock ticker\n",
    "grouped = filtered_data.groupby('Ticker', group_keys=False)\n",
    "\n",
    "# Feature 1: Log Return\n",
    "filtered_data['LogReturn'] = grouped['Close'].apply(lambda x: np.log(x / x.shift(1)))\n",
    "\n",
    "# Feature 2: 10-Day Moving Average of Close\n",
    "filtered_data['MA10'] = grouped['Close'].apply(lambda x: x.rolling(window=10).mean())\n",
    "\n",
    "# Feature 3: 10-Day Volatility (Std Dev of Log Returns)\n",
    "filtered_data['Volatility10'] = grouped['LogReturn'].apply(lambda x: x.rolling(window=10).std())\n",
    "print(filtered_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6874f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay! No more NaNs! \n",
      "\n",
      "<bound method NDFrame.head of                Date    Open     High      Low   Close   Volume  OpenInt  \\\n",
      "10       1999-12-03  30.336  30.8420  29.9090  30.039  3223074        0   \n",
      "11       1999-12-06  30.547  31.3480  30.5050  30.883  2385046        0   \n",
      "12       1999-12-07  30.883  31.0520  29.9090  30.547  2348161        0   \n",
      "13       1999-12-08  30.547  30.7950  30.2490  30.505  2000481        0   \n",
      "14       1999-12-09  30.547  31.0120  30.5470  30.924  2150096        0   \n",
      "...             ...     ...      ...      ...     ...      ...      ...   \n",
      "14887660 2017-11-06  10.420  11.5400  10.4200  11.190   977948        0   \n",
      "14887661 2017-11-07  11.300  11.4200  10.6700  10.830   451210        0   \n",
      "14887662 2017-11-08  10.700  11.0600  10.3500  10.900   336449        0   \n",
      "14887663 2017-11-09  11.000  11.8563  10.9700  11.600   463067        0   \n",
      "14887664 2017-11-10  11.680  13.1500  11.3043  12.460   885587        0   \n",
      "\n",
      "           Ticker  LogReturn     MA10  Volatility10  \n",
      "10           a.us   0.008458  28.5208      0.053895  \n",
      "11           a.us   0.027709  28.8834      0.044701  \n",
      "12           a.us  -0.010939  28.9679      0.036825  \n",
      "13           a.us  -0.001376  29.3182      0.013804  \n",
      "14           a.us   0.013642  29.6389      0.012940  \n",
      "...           ...        ...      ...           ...  \n",
      "14887660  zyne.us   0.079968   9.9970      0.035647  \n",
      "14887661  zyne.us  -0.032700  10.0750      0.038328  \n",
      "14887662  zyne.us   0.006443  10.1650      0.038084  \n",
      "14887663  zyne.us   0.062242  10.3750      0.035041  \n",
      "14887664  zyne.us   0.071518  10.6410      0.038616  \n",
      "\n",
      "[11546824 rows x 11 columns]>\n"
     ]
    }
   ],
   "source": [
    "#At this point you will see that some of the engineered features are NaN.\n",
    "#Why? For a log return, it is because there is no prior close, so it can't calculate the percentage change from one day to the next\n",
    "#For MA10, it's because there must be at least 10 samples to calculate the smoothed average over 10 days. Hence the first 9 entries are NaN\n",
    "#For Volatility10, it needs 11 total entries to perfrom the calculation of degree of variation in returns. \n",
    "\n",
    "#Fix by deleting entries with NaN. Apparently it's 'standard practice' \n",
    "filtered_data = filtered_data.dropna(subset=['LogReturn', 'MA10', 'Volatility10'])\n",
    "print(\"Yay! No more NaNs! \\n\")\n",
    "print(filtered_data.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a195d248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_35468\\206660113.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['RSI14'] = (\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_35468\\206660113.py:38: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_data = filtered_data.groupby('Ticker', group_keys=False).apply(apply_macd)\n"
     ]
    }
   ],
   "source": [
    "#RSI feature for calculating if stock is overbought or oversold\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "filtered_data['RSI14'] = (\n",
    "    filtered_data.groupby('Ticker')['Close'].transform(lambda x: compute_rsi(x, 14))\n",
    ")\n",
    "#Will come back to this and see if it affects the outcome\n",
    "#MACD feature to detect if the market is bear/bull market\n",
    "# --- MACD Feature Function ---\n",
    "def compute_macd(series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    hist = macd - signal_line\n",
    "    return pd.DataFrame({\n",
    "        'MACD': macd,\n",
    "        'MACD_Signal': signal_line,\n",
    "        'MACD_Hist': hist\n",
    "    }, index=series.index)\n",
    "\n",
    "# --- Apply MACD Per Ticker ---\n",
    "# Ensure proper sorting\n",
    "filtered_data = filtered_data.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Apply MACD and attach results to the original rows\n",
    "def apply_macd(group):\n",
    "    macd_result = compute_macd(group['Close']).reset_index(drop=True)\n",
    "    return pd.concat([group.reset_index(drop=True), macd_result], axis=1)\n",
    "\n",
    "filtered_data = filtered_data.groupby('Ticker', group_keys=False).apply(apply_macd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645a77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have features, it's time to create some juicy labels to say if the stock price went up that day. \n",
    "#This can be twofold a problem because we can either make it a classifier(should we sell or not) or a reggressor (how much do we sell + or -)\n",
    "#Classifier to start\n",
    "\n",
    "filtered_data = filtered_data.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# Grouped shift to get next-day close price\n",
    "filtered_data['NextClose'] = filtered_data.groupby('Ticker')['Close'].shift(-1)\n",
    "\n",
    "# Compute next-day log return\n",
    "filtered_data['NextLogReturn'] = np.log(filtered_data['NextClose'] / filtered_data['Close'])\n",
    "\n",
    "# 1 if next day's return is positive, else 0\n",
    "filtered_data['Target'] = (filtered_data['NextLogReturn'] > 0).astype(int)\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['NextLogReturn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a6a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9233213, 18), Test shape: (2309514, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sort data properly\n",
    "filtered_data = filtered_data.sort_values(['Ticker', 'Date']).copy()\n",
    "\n",
    "# Split function for a single ticker's time series\n",
    "def time_split(group, train_ratio=0.8):\n",
    "    n = len(group)\n",
    "    split_idx = int(n * train_ratio)\n",
    "    train = group.iloc[:split_idx]\n",
    "    test = group.iloc[split_idx:]\n",
    "    return train, test\n",
    "\n",
    "# Apply per ticker\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for _, group in filtered_data.groupby('Ticker'):\n",
    "    train_group, test_group = time_split(group)\n",
    "    train_list.append(train_group)\n",
    "    test_list.append(test_group)\n",
    "\n",
    "# Combine all\n",
    "train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01eaf1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'OpenInt', 'Ticker',\n",
      "       'LogReturn', 'MA10', 'Volatility10', 'RSI14', 'MACD', 'MACD_Signal',\n",
      "       'MACD_Hist', 'NextClose', 'NextLogReturn', 'Target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(filtered_data.keys())\n",
    "# Feature columns (use only numeric predictors)\n",
    "#would use these if I got MACD calculation to work earlier on\n",
    "features = ['LogReturn', 'MA10', 'Volatility10', 'RSI14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "#features = ['LogReturn', 'MA10', 'Volatility10', 'RSI14']\n",
    "target = 'Target'\n",
    "\n",
    "# Drop rows with missing values (if any remain)\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "\n",
    "# Create X and y\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05492729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [11:33:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Basic model config — can be tuned later\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Fit to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Serializing the model so it can be used in a web application\n",
    "import pickle\n",
    "\n",
    "# After training\n",
    "with open(\"xgb_stock_model_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047e3295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5197    0.5615    0.5398   1158500\n",
      "           1     0.5197    0.4777    0.4978   1151012\n",
      "\n",
      "    accuracy                         0.5197   2309512\n",
      "   macro avg     0.5197    0.5196    0.5188   2309512\n",
      "weighted avg     0.5197    0.5197    0.5189   2309512\n",
      "\n",
      "Confusion matrix:\n",
      " [[650446 508054]\n",
      " [601177 549835]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8ed8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_35468\\2491608969.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = filtered_data.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[:int(len(x)*0.8)])\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_35468\\2491608969.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_df = filtered_data.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[int(len(x)*0.8):])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.027615\n",
      "MAE: 0.015339\n",
      "R² Score: -0.0034\n"
     ]
    }
   ],
   "source": [
    "#Now will try this with regression\n",
    "\n",
    "# Define features and target\n",
    "features = ['LogReturn', 'MA10', 'Volatility10', 'RSI14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "target = 'NextLogReturn'\n",
    "\n",
    "# Drop rows with missing values\n",
    "filtered_data = filtered_data.dropna(subset=features + [target])\n",
    "\n",
    "# Split into train/test\n",
    "train_df = filtered_data.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[:int(len(x)*0.8)])\n",
    "test_df = filtered_data.groupby('Ticker', group_keys=False).apply(lambda x: x.iloc[int(len(x)*0.8):])\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Serializing the model so it can be used in a web application\n",
    "import pickle\n",
    "\n",
    "# After training\n",
    "with open(\"xgb_stock_model_regressor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97f84577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next-day log return: 0.000388\n",
      "Expected % change: 0.04%\n",
      "Predicted next-day price: $155.06\n",
      "Log return: 0.00038845808\n",
      "PercentChange 0.038850307\n",
      "PredictedPrice:  155.06021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LogReturn': np.float32(0.00038845808),\n",
       " 'PercentChange': np.float32(0.038850307),\n",
       " 'PredictedPrice': np.float32(155.06021)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now predicting for novel data\n",
    "new_data = {\n",
    "    'LogReturn': 0.0045,\n",
    "    'MA10': 153.25,\n",
    "    'Volatility10': 0.0132,\n",
    "    'RSI14': 58.7,\n",
    "    'MACD': 0.12,\n",
    "    'MACD_Signal': 0.10,\n",
    "    'MACD_Hist': 0.02\n",
    "}\n",
    "\n",
    "#Novel data needed to be generated and have the same features as the training data\n",
    "X_new = pd.DataFrame([new_data])\n",
    "\n",
    "#Make the prediction by plugging X_new into the trained model\n",
    "predicted_return = model.predict(X_new)[0]\n",
    "print(f\"Predicted next-day log return: {predicted_return:.6f}\")\n",
    "\n",
    "percent_change = (np.exp(predicted_return) - 1) * 100\n",
    "print(f\"Expected % change: {percent_change:.2f}%\")\n",
    "\n",
    "#If you know todays price\n",
    "current_price = 155.00\n",
    "predicted_price = current_price * np.exp(predicted_return)\n",
    "print(f\"Predicted next-day price: ${predicted_price:.2f}\")\n",
    "\n",
    "#A function to do this for you\n",
    "def predict_next_day_return(model, current_price, engineered_features: dict):\n",
    "    import numpy as np\n",
    "    X = pd.DataFrame([engineered_features])\n",
    "    log_return = model.predict(X)[0]\n",
    "    pct_change = (np.exp(log_return) - 1) * 100\n",
    "    predicted_price = current_price * np.exp(log_return)\n",
    "    print(\"Log return:\", log_return)\n",
    "    print(\"PercentChange\", pct_change)\n",
    "    print(\"PredictedPrice: \", predicted_price)\n",
    "    return {\n",
    "        \"LogReturn\": log_return,\n",
    "        \"PercentChange\": pct_change,\n",
    "        \"PredictedPrice\": predicted_price\n",
    "    }\n",
    "\n",
    "predict_next_day_return(model, 155.00, new_data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
